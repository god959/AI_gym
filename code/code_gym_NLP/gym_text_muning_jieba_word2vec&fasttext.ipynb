{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#導入相關模組\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#讀取jieba所需的自建辭典\n",
    "jieba.load_userdict(\"E:/Python 3.7/pyetl/Demodb0103/gym/data/dict.txt\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取要斷詞的json檔\n",
    "DATASET_DIR = 'E:/Python 3.7/pyetl/Demodb0103/gym/data/gymz.json'\n",
    "with open(DATASET_DIR, encoding='utf8') as f:\n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取停止詞典\n",
    "STOP_WORDS_DIR = 'E:/Python 3.7/pyetl/Demodb0103/gym/stopWords.txt'\n",
    "with open(STOP_WORDS_DIR, encoding='utf8') as f:\n",
    "    stop_words = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#抽出文章的標題與內容\n",
    "content_list = list(map(lambda d: d['content'], dataset))\n",
    "title_list = list(map(lambda d: d['title'], dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#替換不需要的句子，並正規化\n",
    "# start = time.clock()\n",
    "gym_list =[]\n",
    "for i in content_list:\n",
    "    i = re.sub('※ 發信站: 批踢踢實業坊',' ', i)\n",
    "    i = re.sub('※ 文章網址:',' ', i)\n",
    "    i = re.sub('※ 編輯:',' ', i)\n",
    "    i = re.sub('(臺灣)',' ', i)\n",
    "    i = re.sub('來自:',' ', i)\n",
    "    #只保留中文\n",
    "    rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "    i = rule.sub('', i)\n",
    "    gym_list.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將文章正規化，並進行jieba斷詞\n",
    "rule = re.compile(r\"[^\\u4e00-\\u9fa5]\")\n",
    "gym_list = [list(jieba.cut(rule.sub('', speech))) for speech in gym_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "##去除停止詞\n",
    "for idx, speech in enumerate(gym_list):\n",
    "    gym_list[idx] = ' '.join([word for word in speech if word not in stop_words])\n",
    "# end = time.clock()\n",
    "# print('運行時間: ' + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python 3.7\\pyetl\\Demodb0103\\venv\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#Word2Vec\n",
    "# 資料形式為一個文檔各為list元素為字串\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#全文章轉為各list包起來\n",
    "doc_clean1 = [doc.split() for doc in word_s]\n",
    "\n",
    "# 建立模型\n",
    "#window:CBOW下決定Word2Vec一次取多少詞來預測中間詞\n",
    "#min_count:出現次數大於等於min_count的詞，才會納入Word2Vec的詞典中\n",
    "#negative:Negative Sampling的取樣數量，5~20適合小數據，2~5適合大數據\n",
    "# worker=使用多核計算機進行更快的訓練\n",
    "model = Word2Vec(doc_clean1,window=5, negative=5,min_count=1)\n",
    "\n",
    "# # 基於2d PCA擬合數據\n",
    "# X = model[model.wv.vocab]\n",
    "# pca = PCA(n_components=2)\n",
    "# result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('瘦子', 0.9213787913322449),\n",
       " ('胖子', 0.9148010015487671),\n",
       " ('瘦胖子', 0.9103908538818359),\n",
       " ('腿頸', 0.9088810086250305),\n",
       " ('瘦子練壯', 0.9006045460700989),\n",
       " ('死胖子', 0.890883207321167),\n",
       " ('胖', 0.8893124461174011),\n",
       " ('泡芙出', 0.8852933645248413),\n",
       " ('泡芙', 0.8737491369247437),\n",
       " ('肥宅臉', 0.872066855430603)]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#顯示指定詞的關聯詞\n",
    "model2.wv.most_similar('肥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>增肌</th>\n",
       "      <th>cos</th>\n",
       "      <th>肥</th>\n",
       "      <th>cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>減脂</td>\n",
       "      <td>0.931740</td>\n",
       "      <td>瘦子</td>\n",
       "      <td>0.943572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>肪</td>\n",
       "      <td>0.907604</td>\n",
       "      <td>胖子</td>\n",
       "      <td>0.932490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>期</td>\n",
       "      <td>0.878710</td>\n",
       "      <td>胖</td>\n",
       "      <td>0.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>增脂</td>\n",
       "      <td>0.819122</td>\n",
       "      <td>瘦下來</td>\n",
       "      <td>0.894688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>增到</td>\n",
       "      <td>0.818283</td>\n",
       "      <td>肉肉</td>\n",
       "      <td>0.880289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>壯起來</td>\n",
       "      <td>0.804180</td>\n",
       "      <td>變腫</td>\n",
       "      <td>0.873781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>增重</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>紙片人</td>\n",
       "      <td>0.871549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>爬增</td>\n",
       "      <td>0.787592</td>\n",
       "      <td>瘦瘦</td>\n",
       "      <td>0.869552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>拖垮</td>\n",
       "      <td>0.778630</td>\n",
       "      <td>身形</td>\n",
       "      <td>0.867059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>小琉球</td>\n",
       "      <td>0.777449</td>\n",
       "      <td>復</td>\n",
       "      <td>0.861484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    增肌       cos    肥       cos\n",
       "0   減脂  0.931740   瘦子  0.943572\n",
       "1    肪  0.907604   胖子  0.932490\n",
       "2    期  0.878710    胖  0.898200\n",
       "3   增脂  0.819122  瘦下來  0.894688\n",
       "4   增到  0.818283   肉肉  0.880289\n",
       "5  壯起來  0.804180   變腫  0.873781\n",
       "6   增重  0.795238  紙片人  0.871549\n",
       "7   爬增  0.787592   瘦瘦  0.869552\n",
       "8   拖垮  0.778630   身形  0.867059\n",
       "9  小琉球  0.777449    復  0.861484"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#顯示各個指定詞的關聯詞\n",
    "def most_similar(w2v_model, words, topn=10):\n",
    "    similar_df = pd.DataFrame()\n",
    "    for word in words:\n",
    "        try:\n",
    "            similar_words = pd.DataFrame(w2v_model.wv.most_similar(word, topn=topn), columns=[word, 'cos'])\n",
    "            similar_df = pd.concat([similar_df, similar_words], axis=1)\n",
    "        except:\n",
    "            print(word, \"not found in Word2Vec model!\")\n",
    "    return similar_df\n",
    "most_similar(model, ['增肌', '肥'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-265-822b3eabbe03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'增肌'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'word2vec'"
     ]
    }
   ],
   "source": [
    "# import word2vec #需要word2vec模組\n",
    "# indexes = model.cosine(u'增肌')\n",
    "# for index in indexes[0]:\n",
    "#     print (model.vocab[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext\n",
    "from gensim.models import FastText\n",
    "model2 = FastText(doc_clean1,window=5, negative=5,min_count=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
